<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Wenhao Zhan </title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="CV_Wenhao_Zhan.pdf">Resume</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Wenhao Zhan </h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://whzhan99.github.io/index.html"><img src="photos/bio_2025.jpg" alt="alt text" width="200px" height="267px" /></a>&nbsp;</td>
<td align="left"><p><b>Wenhao Zhan</b><br />
whzhan99@outlook.com<br />
San Francisco, CA<br />
<a href="https://scholar.google.com/citations?user=o42MH0MAAAAJ&amp;hl=en">Google Scholar</a> and <a href="https://www.linkedin.com/in/wenhao-zhan-65621621b/">LinkedIn</a></p>
<p>I am an incoming Research Scientist at <a href="https://www.databricks.com/product/artificial-intelligence"><b>Mosaic AI, Databricks</b></a>. Previously, I was a Ph.D. student at <a href="https://ece.princeton.edu/"><b>Princeton University</b></a>, where I was fortunate to be advised by Professor <a href="https://jasondlee88.github.io/index.html"><b>Jason D. Lee</b></a> and <a href="https://yuxinchen2020.github.io/"><b>Yuxin Chen</b></a>. Before that, I received my Bachelor's Degree from <a href="https://www.tsinghua.edu.cn/en/"><b>Tsinghua University</b></a>.<br /></p>
<p><b>Research</b></p>
<ol>
<li><p>Foundations and Applications of Reinforcement Learning</p>
</li>
<li><p>Large Language Model Post-training</p>
</li>
<li><p>Statistics and Optimization</p>
</li>
</ol>
</td></tr></table>
<h2>Publications</h2>
<p><b>(* = equal contribution, + = equal contribution and random order, # = equal contributions and ordered alphabetically)</b></p>
<ol>
<li><p>K. Brantley, M. Chen#, Z. Gao#, J. D. Lee, W. Sun, <b>W. Zhan</b>#, X. Zhang, <a href="https://arxiv.org/abs/2505.20686">"Accelerating RL for LLM Reasoning with Optimal Advantage Regression"</a>, Neurips 2025.</p>
</li>
<li><p><b>W. Zhan</b>, S. Fujimoto, Z. Zhu, J. D. Lee, D. R. Jiang, Y. Efroni, <a href="https://arxiv.org/abs/2410.01101">"Exploiting Structure in Offline Multi-Agent RL: The Benefits of Low Interaction Rank"</a>, ICLR 2025.</p>
</li>
<li><p>A. Huang, <b>W. Zhan</b>, T. Xie, J. D. Lee, W. Sun, A. Krishnamurthy, D. J. Foster, <a href="https://arxiv.org/abs/2407.13399">"Correcting the Mythos of KL-Regularization: Direct Alignment without Overoptimization via Chi-squared Preference Optimization"</a>, ICLR 2025 <b>Spotlight</b>.</p>
</li>
<li><p>Z. Gao, <b>W. Zhan</b>, J. D. Chang, G. Swamy, K. Brantley, J. D. Lee, W. Sun, <a href="https://arxiv.org/abs/2410.04612">"Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF"</a>, ICLR 2025.</p>
</li>
<li><p>J. D. Chang*, <b>W. Zhan</b>*, O. Oertell, K. Brantley, D. Misra, J. D. Lee, W. Sun, <a href="https://arxiv.org/abs/2404.08495">"Dataset Reset Policy Optimization for RLHF"</a>, Preprint.</p>
</li>
<li><p>Z. Gao, J. D. Chang, <b>W. Zhan</b>, O. Oertell, G. Swamy, K. Brantley, T. Joachims, J. A. Bagnell, J. D. Lee, W. Sun, <a href="https://arxiv.org/abs/2404.16767">"REBEL: Reinforcement Learning via Regressing Relative Rewards"</a>, Neurips 2024.</p>
</li>
<li><p>Z. Zhang, <b>W. Zhan</b>, Y. Chen, S. S. Du, J. D. Lee, <a href="https://arxiv.org/abs/2312.05134">"Optimal Multi-Distribution Learning"</a>, COLT 2024.</p>
</li>
<li><p><b>W. Zhan</b>, M. Uehara, W. Sun, J. D. Lee, <a href="https://arxiv.org/abs/2305.18505">"Provable Reward-Agnostic Preference-Based Reinforcement Learning"</a>, ICLR 2024 <b>Spotlight</b>.</p>
</li>
<li><p><b>W. Zhan</b>*, M. Uehara*, N. Kallus, J. D. Lee, W. Sun, <a href="https://arxiv.org/abs/2305.14816">"Provable Offline Preference-Based Reinforcement Learning"</a>, ICLR 2024 <b>Spotlight</b>.</p>
</li>
<li><p>Y. Zhao+, <b>W. Zhan</b>+, X. Hu+, H. Leung, F. Farnia, W. Sun, J. D. Lee, <a href="https://arxiv.org/abs/2311.11965">"Provably Efficient CVaR RL in Low-rank MDPs"</a>, ICLR 2024.</p>
</li>
<li><p>G. Li*, <b>W. Zhan</b>*, J. D. Lee, Y. Chi, Y. Chen, <a href="https://arxiv.org/abs/2305.10282">"Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning"</a>, Neurips 2023. </p>
</li>
<li><p><b>W. Zhan</b>*, S. Cen*, B. Huang, Y. Chen, J. D. Lee, Y. Chi, <a href="https://arxiv.org/abs/2105.11066">"Policy Mirror Descent for Regularized Reinforcement Learning: A Generalized Framework with Linear Convergence"</a>, SIAM Journal on Optimization, 2023.</p>
</li>
<li><p><b>W. Zhan</b>, M. Uehara, W. Sun, J. D. Lee, <a href="https://arxiv.org/abs/2207.05738">"PAC Reinforcement Learning for Predictive State Representations"</a>, ICLR 2023.</p>
</li>
<li><p><b>W. Zhan</b>, J. D. Lee, Z. Yang, <a href="https://arxiv.org/abs/2206.01588">"Decentralized Optimistic Hyperpolicy Mirror Descent: Provably No-Regret Learning in Markov Games"</a>, ICLR 2023.</p>
</li>
<li><p><b>W. Zhan</b>, B. Huang, A. Huang, N. Jiang, J. D. Lee, <a href="https://arxiv.org/abs/2202.04634">"Offline Reinforcement Learning with Realizability and Single-policy Concentrability"</a>, COLT 2022.</p>
</li>
<li><p>C. Z. Lee, L. P. Barnes, <b>W. Zhan</b>, A. Özgür, <a href="https://ieeexplore.ieee.org/document/9685768">"Over-the-Air Statistical Estimation of Sparse Models"</a>, GLOBECOM 2021.</p>
</li>
<li><p><b>W. Zhan</b>, H. Tang, J. Wang, <a href="https://ieeexplore.ieee.org/abstract/document/9379559">"Delay Optimal Cross-Layer Scheduling Over Markov Channels with Power Constraint"</a>, BMSB 2020.</p>
</li>
</ol>
<h2>Working</h2>
<h3>Mosaic AI, Databricks</h3>
<p><b>Research Scientist</b><br /> 
Jan 2026 &ndash; <br />
<i>Reinforcement Learning and Large Language Model Post-training</i></p>
<h3>GenAI, Meta</h3>
<p><b>Research Intern</b><br /> 
Jun 2025 &ndash; Sep 2025 <br />
<i>Reinforcement Learning for Tool-Integrated Reasoning Models</i></p>
<h3>Ranking, Meta</h3>
<p><b>Research Intern</b><br /> 
May 2024 &ndash; Oct 2024 <br />
<i>Efficient Multi-Agent Offline Reinforcement Learning</i></p>
<h2>Teaching</h2>
<ul>
<li><p>Spring 2024: Foundations of Reinforcement Learning, as TA (Princeton, Instructor: Prof. Chi Jin).</p>
</li>
<li><p>Fall 2022: Theory of Weakly Supervised Learning, as TA (Princeton, Instructor: Prof. Jason D. Lee).</p>
</li>
</ul>
<h2>Honors</h2>
<ul>
<li><p>2024 Award for Excellence awarded by Princeton SEAS</p>
</li>
<li><p>Honorable mention for the 2023 Jane Street Graduate Research Fellowship</p>
</li>
</ul>
<h2>Talk</h2>
<ul>
<li><p><b>Optimal Multi-Distribution Learning</b><br />
<i>Adaptive Learning in Complex Environments, TTIC Chicago Summer Workshop 2024</i></p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
