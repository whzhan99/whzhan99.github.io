<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Wenhao Zhan </title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="CV_Wenhao_Zhan.pdf">Resume</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Wenhao Zhan </h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://whzhan99.github.io/index.html"><img src="photos/mimikyu.jpg" alt="alt text" width="150px" height="200px" /></a>&nbsp;</td>
<td align="left"><p>I am a Ph.D. student at <a href="https://ece.princeton.edu/">Princeton University</a> advised by Professor <a href="https://jasondlee88.github.io/index.html">Jason D. Lee</a> and <a href="https://yuxinchen2020.github.io/">Yuxin Chen</a>.<br />
Before that, I received my Bachelor's Degree in Electronic Engineering from <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>.<br />
<br />
<b>Office</b>: Friend Center 307, Princeton, NJ.<br />
<b>Email</b>: wenhao.zhan@princeton.edu</p>
</td></tr></table>
<h2>Research</h2>
<p>My research interests include </p>
<ul>
<li><p>Reinforcement Learning</p>
</li>
<li><p>Statistics</p>
</li>
</ul>
<h3>Publications</h3>
<p><b>(* = equal contribution, + = equal contribution and random order)</b></p>
<ol>
<li><p>Z. Zhang, W. Zhan, Y. Chen, S. S. Du, J. D. Lee, <a href="https://arxiv.org/abs/2312.05134">"Optimal Multi-Distribution Learning"</a>, Preprint.</p>
</li>
<li><p>W. Zhan, M. Uehara, W. Sun, J. D. Lee, <a href="https://arxiv.org/abs/2305.18505">"Provable Reward-Agnostic Preference-Based Reinforcement Learning"</a>, accepted to ICLR 2024 <b>Spotlight</b>.</p>
</li>
<li><p>W. Zhan*, M. Uehara*, N. Kallus, J. D. Lee, W. Sun, <a href="https://arxiv.org/abs/2305.14816">"Provable Offline Preference-Based Reinforcement Learning"</a>, accepted to ICLR 2024 <b>Spotlight</b>.</p>
</li>
<li><p>Y. Zhao+, W. Zhan+, X. Hu+, H. Leung, F. Farnia, W. Sun, J. D. Lee, <a href="https://arxiv.org/abs/2311.11965">"Provably Efficient CVaR RL in Low-rank MDPs"</a>, accepted to ICLR 2024.</p>
</li>
<li><p>G. Li*, W. Zhan*, J. D. Lee, Y. Chi, Y. Chen, <a href="https://arxiv.org/abs/2305.10282">"Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning"</a>, Neurips 2023. </p>
</li>
<li><p>W. Zhan*, S. Cen*, B. Huang, Y. Chen, J. D. Lee, Y. Chi, <a href="https://arxiv.org/abs/2105.11066">"Policy Mirror Descent for Regularized Reinforcement Learning: A Generalized Framework with Linear Convergence"</a>, SIAM Journal on Optimization, 2023.</p>
</li>
<li><p>W. Zhan, M. Uehara, W. Sun, J. D. Lee, <a href="https://arxiv.org/abs/2207.05738">"PAC Reinforcement Learning for Predictive State Representations"</a>, ICLR 2023.</p>
</li>
<li><p>W. Zhan, J. D. Lee, Z. Yang, <a href="https://arxiv.org/abs/2206.01588">"Decentralized Optimistic Hyperpolicy Mirror Descent: Provably No-Regret Learning in Markov Games"</a>, ICLR 2023.</p>
</li>
<li><p>W. Zhan, B. Huang, A. Huang, N. Jiang, J. D. Lee, <a href="https://arxiv.org/abs/2202.04634">"Offline Reinforcement Learning with Realizability and Single-policy Concentrability"</a>, COLT 2022.</p>
</li>
<li><p>C. Z. Lee, L. P. Barnes, W. Zhan, A. Özgür, <a href="https://ieeexplore.ieee.org/document/9685768">"Over-the-Air Statistical Estimation of Sparse Models"</a>, GLOBECOM 2021.</p>
</li>
<li><p>W. Zhan, H. Tang, J. Wang, <a href="https://ieeexplore.ieee.org/abstract/document/9379559">"Delay Optimal Cross-Layer Scheduling Over Markov Channels with Power Constraint"</a>, BMSB 2020.</p>
</li>
</ol>
<h2>Teaching</h2>
<ul>
<li><p>Spring 2024: Foundations of Reinforcement Learning, as TA (Princeton, Instructor: Prof. Chi Jin).</p>
</li>
<li><p>Fall 2022: Theory of Weakly Supervised Learning, as TA (Princeton, Instructor: Prof. Jason D. Lee).</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
